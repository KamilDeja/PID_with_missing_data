{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include source package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to the project directory\n",
    "%cd ..\n",
    "# working directory should be ../FSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath('src')\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pdi.data.preparation import FeatureSetPreparation, MeanImputation, DeletePreparation, RegressionImputation, EnsemblePreparation\n",
    "from pdi.constants import PARTICLES_DICT, TARGET_CODES, NUM_WORKERS, P_RANGE, P_RESOLUTION\n",
    "from pdi.models import NeuralNet, NeuralNetEnsemble, AttentionModel\n",
    "from pdi.data.types import Split, Additional\n",
    "from pdi.evaluate import calculate_precision_recall, get_predictions_data_and_loss\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    \"Mean\": {\n",
    "        \"model_class\": NeuralNet,\n",
    "        \"data\": {\n",
    "            \"all\": MeanImputation,\n",
    "            \"complete_only\": DeletePreparation,\n",
    "        }\n",
    "    },\n",
    "    \"Regression\": {\n",
    "        \"model_class\": NeuralNet,\n",
    "        \"data\": {\n",
    "            \"all\": RegressionImputation,\n",
    "            \"complete_only\": DeletePreparation,\n",
    "        }\n",
    "    },\n",
    "    \"Ensemble\": {\n",
    "        \"model_class\": NeuralNetEnsemble,\n",
    "        \"data\": {\n",
    "            \"all\": EnsemblePreparation,\n",
    "            \"complete_only\": lambda: EnsemblePreparation(complete_only=True),\n",
    "        }\n",
    "    },\n",
    "    \"Proposed\": {\n",
    "        \"model_class\": AttentionModel,\n",
    "        \"data\": {\n",
    "            \"all\": FeatureSetPreparation,\n",
    "            \"complete_only\": lambda: FeatureSetPreparation(complete_only=True),\n",
    "        }\n",
    "    },\n",
    "    \"Delete\": {\n",
    "        \"model_class\": NeuralNet,\n",
    "        \"data\": {\n",
    "            \"complete_only\": DeletePreparation,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "RANGES_95 = {\n",
    "    211: [0.118081, 1.605047],\n",
    "    2212: [0.257578, 2.662042],\n",
    "    321: [0.175148, 2.520606],\n",
    "    -211: [0.119654, 1.604230],\n",
    "    -2212: [0.265045, 2.746735],\n",
    "    -321: [0.185005, 2.538974],\n",
    "}\n",
    "\n",
    "RANGES_90 = {\n",
    "    211: [0.118081, 1.204883],\n",
    "    2212: [0.257578, 2.152858],\n",
    "    321: [0.175148, 1.976095],\n",
    "    -211: [0.119654, 1.203556],\n",
    "    -2212: [0.265045, 2.204669],\n",
    "    -321: [0.185005, 1.977073],\n",
    "}\n",
    "\n",
    "particle_names = [PARTICLES_DICT[i] for i in TARGET_CODES]\n",
    "model_names = EXPERIMENTS.keys()\n",
    "metrics = [\"precision\", \"recall\", \"f1\"]\n",
    "data_types = [\"all\", \"complete_only\"]\n",
    "pt_percentage = [\"90%\", \"95%\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate pretrained models on test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from os.path import isfile\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "SAMPLES = [\"\", \"_0\", \"_1\", \"_2\"]\n",
    "\n",
    "for sample in SAMPLES:\n",
    "    prediction_data = {}\n",
    "    save_path = f\"reports/test_results{sample}.pkl\"\n",
    "    if isfile(save_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"Testing: {sample}\")\n",
    "\n",
    "    for target_code in TARGET_CODES:\n",
    "        prediction_data[target_code] = {}\n",
    "        particle_name = PARTICLES_DICT[target_code]\n",
    "        for experiment_name, exp_dict in EXPERIMENTS.items():\n",
    "            load_path = f\"models/{experiment_name}/{particle_name}{sample}.pt\"\n",
    "            saved_model = torch.load(load_path)\n",
    "            model = exp_dict[\"model_class\"](*saved_model[\"model_args\"]).to(device)\n",
    "            model.thres = saved_model[\"model_thres\"]\n",
    "            model.load_state_dict(saved_model[\"state_dict\"])\n",
    "\n",
    "            batch_size = 512\n",
    "\n",
    "            prediction_data[target_code][experiment_name] = {}\n",
    "            for data_type, data_prep in exp_dict[\"data\"].items():\n",
    "                test_loader, = data_prep().prepare_dataloaders(batch_size, NUM_WORKERS, [Split.TEST])\n",
    "\n",
    "                predictions, targets, add_data, _ = get_predictions_data_and_loss(model, test_loader, device)\n",
    "\n",
    "                selected = predictions > model.thres\n",
    "                binary_targets = targets == target_code\n",
    "                \n",
    "                prediction_data[target_code][experiment_name][data_type] = {\n",
    "                    \"targets\": binary_targets,\n",
    "                    \"predictions\": predictions,\n",
    "                    \"momentum\": add_data[Additional.fPt.name],\n",
    "                    \"threshold\": model.thres\n",
    "                }\n",
    "\n",
    "\n",
    "            \n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(prediction_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "prediction_data = []\n",
    "SAMPLES = [\"\", \"_0\", \"_1\", \"_2\"]\n",
    "\n",
    "for sample in SAMPLES:\n",
    "    with open(f\"reports/test_results{sample}.pkl\", \"rb\") as f:\n",
    "        prediction_data.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot precision (purity) and recall (efficiency) comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pdi.visualise as vis\n",
    "from importlib import reload\n",
    "reload(vis)\n",
    "\n",
    "p_min, p_max = P_RANGE\n",
    "p_range = np.linspace(p_min, p_max, P_RESOLUTION)\n",
    "intervals = list(zip(p_range[:-1], p_range[1:]))\n",
    "\n",
    "for target_code in TARGET_CODES:\n",
    "    particle_name = PARTICLES_DICT[target_code]\n",
    "    for data_type in data_types:\n",
    "        data = {}\n",
    "        for exp_name, exp_dict in prediction_data[0][target_code].items():\n",
    "            if exp_name == \"Proposed\":\n",
    "                exp_name = \"FSE + attention\"\n",
    "            if data_type in exp_dict:\n",
    "                data[exp_name] = exp_dict[data_type]\n",
    "\n",
    "        save_dir = f\"reports/figures/comparison_{data_type}/{particle_name}\"   \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # plot_purity_comparison(particle_name, data, intervals, save_dir)\n",
    "        # plot_efficiency_comparison(particle_name, data, intervals, save_dir)\n",
    "        \n",
    "        vis.plot_precision_recall_comparison(particle_name, data, RANGES_90[target_code], 90, save_dir)\n",
    "        vis.plot_precision_recall_comparison(particle_name, data, RANGES_95[target_code], 95, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_results = pd.DataFrame(\n",
    "    index=pd.MultiIndex.from_product(\n",
    "        [pt_percentage, particle_names, model_names], names=[\"pt_range\", \"particle\", \"model\"]\n",
    "        ),\n",
    "    columns=pd.MultiIndex.from_product(\n",
    "        [list(range(len(SAMPLES))), data_types, metrics], names=[\"test_case\", \"data\", \"metric\"]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "for i in range(len(SAMPLES)):\n",
    "    for target_code in TARGET_CODES:\n",
    "        for pt_percent, pt_range in zip(pt_percentage, [RANGES_90[target_code], RANGES_95[target_code]]):\n",
    "            particle_name = PARTICLES_DICT[target_code]\n",
    "            for data_type in data_types:\n",
    "                data = {}\n",
    "                for exp_name, exp_dict in prediction_data[i][target_code].items():\n",
    "                    if data_type in exp_dict:\n",
    "                        data[exp_name] = exp_dict[data_type]\n",
    "\n",
    "                for method_name, results in data.items():\n",
    "                    targets = results[\"targets\"]\n",
    "                    preds = results[\"predictions\"]\n",
    "                    momentum = results[\"momentum\"]\n",
    "                    thres = results[\"threshold\"]\n",
    "\n",
    "                    mask = (momentum >= pt_range[0]) & (momentum <= pt_range[1])\n",
    "                    targets = targets[mask]\n",
    "                    preds = preds[mask]\n",
    "\n",
    "                    selected = preds > thres\n",
    "        \n",
    "                    true_positives = int(np.sum(selected & targets))\n",
    "                    selected_positives = int(np.sum(selected))\n",
    "                    positives = int(np.sum(targets))\n",
    "\n",
    "                    precision, recall, _, _ = calculate_precision_recall(true_positives, selected_positives, positives)\n",
    "                    f1 = 2 * precision * recall / (precision + recall + np.finfo(float).eps)\n",
    "\n",
    "                    metric_results.loc[(pt_percent, particle_name, method_name), (i, data_type)] = precision, recall, f1\n",
    "                \n",
    "os.makedirs(\"reports/tables\", exist_ok=True)\n",
    "metric_results.to_csv(f\"reports/tables/comparison_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create LaTeX table from metrics file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"reports/tables/comparison_metrics.csv\",\n",
    "                 index_col=[0, 1, 2],\n",
    "                 header=[0, 1, 2])\n",
    "subsets = [((part, slice(None)), column) for column in df.columns\n",
    "           for part in particle_names]\n",
    "\n",
    "df = df.rename(columns={\"f1\": \"$F_1$\"})\n",
    "\n",
    "def bold_max(x):\n",
    "    arr = np.array(list(x))[:, 0]\n",
    "    return np.where(arr == np.nanmax(arr), f\"textbf:--rwrap\", None)\n",
    "\n",
    "\n",
    "for percent in pt_percentage:\n",
    "    for dt in data_types:\n",
    "        save_dir = f\"reports/tables/{percent[:-1]}/comparison_{dt}\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        for particle in particle_names:\n",
    "            results = df.xs((percent, particle)).xs(dt, axis='columns', level=1).groupby(level=\"metric\", axis=\"columns\")\n",
    "\n",
    "            results_mean = results.mean() * 100\n",
    "            results_std = results.std(numeric_only=True) * 100\n",
    "\n",
    "            combined = pd.concat([results_mean, results_std], keys=['mean', 'std'], axis=1)\n",
    "            combined = combined.reorder_levels([1, 0], axis=1).stack().groupby(\"model\").agg(list)\n",
    "            combined = combined[[\"precision\", \"recall\", \"$F_1$\"]]\n",
    "\n",
    "            columns = [(slice(None), (column)) for column in metrics]\n",
    "            \n",
    "            style = combined.style\n",
    "\n",
    "            for column in columns:\n",
    "                if column[1] == \"f1\":\n",
    "                    column = (slice(None), \"$F_1$\")\n",
    "                style.apply(bold_max, subset=column)\n",
    "                # style = style.highlight_max(column, props='textbf:--rwrap')\n",
    "\n",
    "            style.format('{0[0]:.2f} $\\pm$ {0[1]:.2f}\\%')\n",
    "\n",
    "\n",
    "            style.to_latex(f\"{save_dir}/{particle}_results.tex\",\n",
    "                        hrules=True,\n",
    "                        clines=\"all;data\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ddece17a642146cc49b2b032ef0865aafdc2c2bbdb5ddaf5cd80c99ab7aea91"
  },
  "kernelspec": {
   "display_name": "pdi-venv",
   "language": "python",
   "name": "pdi-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
